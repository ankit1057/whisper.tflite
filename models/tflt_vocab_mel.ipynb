{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/whisper.cpp.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJlEnlrp2jou",
        "outputId": "f5e68674-a062-4395-93c5-adb27cf3e46b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'whisper.cpp'...\n",
            "remote: Enumerating objects: 4782, done.\u001b[K\n",
            "remote: Total 4782 (delta 0), reused 0 (delta 0), pack-reused 4782\u001b[K\n",
            "Receiving objects: 100% (4782/4782), 8.04 MiB | 18.76 MiB/s, done.\n",
            "Resolving deltas: 100% (3052/3052), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBtHC4J13F-z",
        "outputId": "885a8640-38f9-46a8-e5b1-3e1091ef1efc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'whisper'...\n",
            "remote: Enumerating objects: 619, done.\u001b[K\n",
            "remote: Counting objects: 100% (338/338), done.\u001b[K\n",
            "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
            "remote: Total 619 (delta 292), reused 269 (delta 253), pack-reused 281\u001b[K\n",
            "Receiving objects: 100% (619/619), 8.16 MiB | 23.74 MiB/s, done.\n",
            "Resolving deltas: 100% (369/369), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://openaipublic.azureedge.net/main/whisper/models/65147644a518d12f04e32d6f3b26facc3f8dd46e5390956a9424a650c0ce22b9/tiny.pt\n",
        "!wget https://openaipublic.azureedge.net/main/whisper/models/d3dd57d32accea0b295c96e26691aa14d8822fac7d9d27d5dc00b4ca2826dd03/tiny.en.pt"
      ],
      "metadata": {
        "id": "1GitOcjn7nq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import sys\n",
        "import json\n",
        "import struct\n",
        "import base64\n",
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def bytes_to_unicode():\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1)) + list(range(ord(\"¡\"), ord(\"¬\")+1)) + list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8 + n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "fname_inp = \"/content/tiny.en.pt\"\n",
        "dir_whisper = \"/content/whisper\"\n",
        "dir_out = \"/content/\"\n",
        "\n",
        "# try to load PyTorch binary data\n",
        "try:\n",
        "    model_bytes = open(fname_inp, \"rb\").read()\n",
        "    with io.BytesIO(model_bytes) as fp:\n",
        "        checkpoint = torch.load(fp, map_location=\"cpu\")\n",
        "except Exception:\n",
        "    print(\"Error: failed to load PyTorch model file:\", fname_inp)\n",
        "    sys.exit(1)\n",
        "\n",
        "hparams = checkpoint[\"dims\"]\n",
        "print(\"hparams:\", hparams)\n",
        "\n",
        "list_vars = checkpoint[\"model_state_dict\"]\n",
        "\n",
        "# load mel filters\n",
        "n_mels = hparams[\"n_mels\"]\n",
        "with np.load(Path(dir_whisper) / \"whisper\" / \"assets\" / \"mel_filters.npz\") as f:\n",
        "    filters = torch.from_numpy(f[f\"mel_{n_mels}\"])\n",
        "\n",
        "# load tokenizer\n",
        "multilingual = hparams[\"n_vocab\"] == 51865\n",
        "tokenizer = Path(dir_whisper) / \"whisper\" / \"assets\" / (multilingual and \"multilingual.tiktoken\" or \"gpt2.tiktoken\")\n",
        "tokenizer_type = \"tiktoken\"\n",
        "if not tokenizer.is_file():\n",
        "    tokenizer = Path(dir_whisper) / \"whisper\" / \"assets\" / (multilingual and \"multilingual\" or \"gpt2\") / \"vocab.json\"\n",
        "    tokenizer_type = \"hf_transformers\"\n",
        "    if not tokenizer.is_file():\n",
        "        print(\"Error: failed to find either tiktoken or hf_transformers tokenizer file:\", tokenizer)\n",
        "        sys.exit(1)\n",
        "\n",
        "byte_encoder = bytes_to_unicode()\n",
        "byte_decoder = {v: k for k, v in byte_encoder.items()}\n",
        "\n",
        "if tokenizer_type == \"tiktoken\":\n",
        "    with open(tokenizer, \"rb\") as f:\n",
        "        contents = f.read()\n",
        "        tokens = {base64.b64decode(token): int(rank) for token, rank in (line.split() for line in contents.splitlines() if line)}\n",
        "elif tokenizer_type == \"hf_transformers\":\n",
        "    with open(tokenizer, \"r\", encoding=\"utf8\") as f:\n",
        "        _tokens_raw = json.load(f)\n",
        "        if '<|endoftext|>' in _tokens_raw:\n",
        "            del _tokens_raw['<|endoftext|>']\n",
        "        tokens = {bytes([byte_decoder[c] for c in token]): int(idx) for token, idx in _tokens_raw.items()}\n",
        "\n",
        "# output in the same directory as the model\n",
        "fname_out = Path(dir_out) / \"tflt-vocab-mel.bin\"\n",
        "\n",
        "fout = fname_out.open(\"wb\")\n",
        "\n",
        "fout.write(struct.pack(\"i\", 0x74666C74))\n",
        "# write mel filters\n",
        "fout.write(struct.pack(\"i\", filters.shape[0]))\n",
        "fout.write(struct.pack(\"i\", filters.shape[1]))\n",
        "for i in range(filters.shape[0]):\n",
        "    for j in range(filters.shape[1]):\n",
        "        fout.write(struct.pack(\"f\", filters[i][j]))\n",
        "\n",
        "# write tokenizer\n",
        "fout.write(struct.pack(\"i\", len(tokens)))\n",
        "\n",
        "for key in tokens:\n",
        "    fout.write(struct.pack(\"i\", len(key)))\n",
        "    fout.write(key)\n",
        "\n",
        "fout.close()\n",
        "\n",
        "print(\"Done. Output file: \" , fname_out)\n",
        "print(\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rpk1g6wm9q-z",
        "outputId": "9ff8e4b6-c633-4691-ba5e-32ffbb63cbe3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hparams: {'n_mels': 80, 'n_vocab': 51864, 'n_audio_ctx': 1500, 'n_audio_state': 384, 'n_audio_head': 6, 'n_audio_layer': 4, 'n_text_ctx': 448, 'n_text_state': 384, 'n_text_head': 6, 'n_text_layer': 4}\n",
            "Done. Output file:  /content/tflt-vocab-mel.bin\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -la"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alemf71u-dkO",
        "outputId": "fbcd11f3-d51b-4a11-f36f-1aafe12e5719"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 148224\n",
            "drwxr-xr-x  1 root root     4096 Oct 25 03:54 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x  1 root root     4096 Oct 25 03:12 \u001b[01;34m..\u001b[0m/\n",
            "drwxr-xr-x  4 root root     4096 Oct 23 13:26 \u001b[01;34m.config\u001b[0m/\n",
            "drwxr-xr-x  2 root root     4096 Oct 25 03:22 \u001b[01;34m.ipynb_checkpoints\u001b[0m/\n",
            "drwxr-xr-x  1 root root     4096 Oct 23 13:26 \u001b[01;34msample_data\u001b[0m/\n",
            "-rw-r--r--  1 root root        7 Oct 25 03:24 test.py\n",
            "-rw-r--r--  1 root root   586174 Oct 25 04:03 tflt-vocab-mel.bin\n",
            "-rw-r--r--  1 root root 75571315 Sep 20  2022 tiny.en.pt\n",
            "-rw-r--r--  1 root root 75572083 Sep 20  2022 tiny.pt\n",
            "drwxr-xr-x  8 root root     4096 Oct 25 03:42 \u001b[01;34mwhisper\u001b[0m/\n",
            "drwxr-xr-x 14 root root     4096 Oct 25 03:22 \u001b[01;34mwhisper.cpp\u001b[0m/\n"
          ]
        }
      ]
    }
  ]
}